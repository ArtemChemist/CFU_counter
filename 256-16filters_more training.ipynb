{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import csv\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import batch_normalization\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folder\n",
    "SIZE = 128 #Size of the images this model will deal with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folder, unless it is already set up\n",
    "p = os.path.abspath('.')\n",
    "output_dir = os.path.join(p, f'{SIZE}')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    print(f\"{output_dir} created\")\n",
    "else:\n",
    "    print(f\"{output_dir} already exists\")\n",
    "output_dir = output_dir\n",
    "\n",
    "# Resize images and write them into folder, unless they are alrady there\n",
    "for file in glob.glob('./Thresholded/*'):\n",
    "\tfilename = file.split('/')[-1]\n",
    "\tnew_name = os.path.join(output_dir, filename)\n",
    "\tif not os.path.exists(new_name):\n",
    "\t\timg = cv2.imread(file, cv2.IMREAD_COLOR)\n",
    "\t\timg = cv2.resize(img, (SIZE,SIZE))\n",
    "\t\tcv2.imwrite(new_name, img)\n",
    "\n",
    "# Read filenames in the folder\n",
    "image_names = []\n",
    "labels = []\n",
    "\n",
    "for file in glob.glob(f'{output_dir}/*.jpg'):\n",
    "\tfilename = file.split('/')[-1]\n",
    "\tnum_colonies = int(filename.split('-')[2].strip().split('.')[0])\n",
    "\tif num_colonies == 0:\n",
    "\t\tlabels.append(0)\n",
    "\telse:\n",
    "\t\tlabels.append(1)\n",
    "\timage_names.append(filename)\n",
    "\n",
    "image_names = np.array(image_names) #  Array with image nbames\n",
    "labels = np.array(labels)\t\t\t#  Array with binary labels\n",
    "\n",
    "#  Make two arrays with filenames: one for positive and one for negative images\n",
    "pos_im_files = [image_names[i] for i in range(image_names.shape[0]) if labels[i]]\n",
    "pos_im_files = np.array(pos_im_files)\n",
    "neg_im_files = [image_names[i] for i in range(image_names.shape[0]) if not labels[i]]\n",
    "neg_im_files = np.array(neg_im_files)\n",
    "num_pos = pos_im_files.shape[0]\n",
    "num_neg = neg_im_files.shape[0]\n",
    "print(f'Full dataset: {num_pos} images with bacteria and {num_neg} without')\n",
    "\n",
    "#  There are ~10x positive than negative.\n",
    "#  For initital training let's get all positive and equal number of negative\n",
    "#  For that:\n",
    "#  (1) Index files of randomly selected negative images\n",
    "np.random.seed(0) # For reproducibility\n",
    "idx = np.random.choice(np.arange(neg_im_files.shape[0]), num_pos, replace = False)\n",
    "#  (2) Apply this index to the array of image names\n",
    "neg_selected_files = neg_im_files[idx]\n",
    "#  (3) Make a full set of file names, with X and y, not split into train.test yet\n",
    "X_files = np.concatenate((pos_im_files,neg_selected_files), axis =0)\n",
    "y = np.concatenate((np.ones(num_pos), np.zeros(neg_selected_files.shape[0])))\n",
    "\n",
    "print(f\"Balanced dataset: X: {X_files.shape}, y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now shuffle the names\n",
    "print(\"Suffling the dataset ....\")\n",
    "num_datapoints = X_files.shape[0]\n",
    "indices = np.arange(num_datapoints)\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "X_shuffled_files = X_files[shuffled_indices]\n",
    "y_shuffled = y[shuffled_indices]\n",
    "\n",
    "# Read the actual files:\n",
    "print(\"Reading actual images...\")\n",
    "images = []\n",
    "for file_name in X_shuffled_files:\n",
    "    file = os.path.join(output_dir, file_name)\n",
    "    img = cv2.imread(file, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    images.append(img)\n",
    "\n",
    "X_shuffled = np.array(images) # Shuffled dataset\n",
    "\n",
    "# Split into test and train. \n",
    "# Rely on random shuffling, can just take first 70% for train\n",
    "print(\"Splitting to train/test 70/30\")\n",
    "num_train = int(num_datapoints*0.7)\n",
    "\n",
    "x_train = X_shuffled[ :num_train, : , : , : ]\n",
    "y_train = y_shuffled[ :num_train]\n",
    "x_test = X_shuffled[num_train: , : , : , : ]\n",
    "y_test = y_shuffled[num_train: ]\n",
    "\n",
    "print(f'Set shape: train-> {x_train.shape}, test ->{x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 25 images to make sure it makes sense\n",
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(12,12))\n",
    "\n",
    "row = 0\n",
    "label_name = \"Shirt predicted as coat\"\n",
    "axs[row,0].set_title(label_name)\n",
    "axs[row,0].axis('off')\n",
    "\n",
    "for i, j in [(i,j) for i in np.arange(5) for j in np.arange(5)]:\n",
    "    idx = 5*i+j\n",
    "    image = cv2.cvtColor(x_train[idx], cv2.COLOR_HSV2RGB)\n",
    "    axs[i,j].imshow(image)\n",
    "    axs[i,j].set_title(y_train[idx])\n",
    "    axs[i,j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make convolutional layers\n",
    "\n",
    "kernels = [ (np.max([int(SIZE*fraction), 3]),np.max([int(SIZE*fraction), 3])) for fraction in (0.01, 0.02, 0.04, 0.06)]\n",
    "\n",
    "activation = 'sigmoid'\n",
    "\n",
    "feature_extractor = Sequential()\n",
    "feature_extractor.add(Conv2D(16, kernels[0], activation = activation, padding = 'same', input_shape = (SIZE, SIZE, 3)))\n",
    "feature_extractor.add(batch_normalization.BatchNormalization())\n",
    "\n",
    "for kernel in kernels[1:]:\n",
    "    print(kernel)\n",
    "    feature_extractor.add(Conv2D(16, kernel, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "    feature_extractor.add(batch_normalization.BatchNormalization())\n",
    "    feature_extractor.add(MaxPooling2D())\n",
    "\n",
    "feature_extractor.add(Flatten())\n",
    "\n",
    "#Add layers for deep learning prediction\n",
    "x = feature_extractor.output  \n",
    "x = Dense(128, activation = activation, kernel_initializer = 'he_uniform')(x)\n",
    "prediction_layer = Dense(2, activation = 'softmax')(x)\n",
    "\n",
    "# Make a new model combining both feature extractor and x\n",
    "cnn_model = Model(inputs=feature_extractor.input, outputs=prediction_layer)\n",
    "cnn_model.compile(optimizer='rmsprop',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "print(cnn_model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class EarlyStoppingAtAccuracy(Callback):\n",
    "    def __init__(self, threshold, acc_hist):\n",
    "        super(EarlyStoppingAtAccuracy, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.acc_hist = acc_hist\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_accuracy = logs.get(\"val_accuracy\")\n",
    "        self.acc_hist.append(val_accuracy)\n",
    "        if len(self.acc_hist) > 3 and np.median(self.acc_hist[-2:]) >= self.threshold:\n",
    "            self.model.stop_training = True\n",
    "            print(f\"\\nStopping training: Validation accuracy reached {self.threshold:.2f}\")\n",
    "\n",
    "# Set the desired accuracy threshold\n",
    "accuracy_threshold = 0.7\n",
    "acc_hist = []\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping = EarlyStoppingAtAccuracy(accuracy_threshold, acc_hist )\n",
    "\n",
    "\n",
    "#Convert y to one hot encoding for use in CNN \n",
    "from keras.utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Train CNN model\n",
    "history = cnn_model.fit(x_train, y_train_one_hot, epochs=100, validation_data = (x_test, y_test_one_hot), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folder, unless it is already set up\n",
    "p = os.path.abspath('.')\n",
    "model_dir = os.path.join(p, f'{SIZE}','model')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    print(f\"{model_dir} created\")\n",
    "else:\n",
    "    print(f\"{model_dir} already exists\")\n",
    "\n",
    "cnn_model.save(model_dir)\n",
    "\n",
    "# Save history into a csv\n",
    "with open(model_dir, 'w+') as file:\n",
    "  csvwriter = csv.writer(file)\n",
    "  csvwriter.writerow('val_loss')\n",
    "  csvwriter.writerow(history.history['val_loss'])\n",
    "  csvwriter.writerow('loss')\n",
    "  csvwriter.writerow(history.history['loss'])\n",
    "  csvwriter.writerow('val_accuracy')\n",
    "  csvwriter.writerow(history.history['val_accuracy'])\n",
    "  csvwriter.writerow('accuracy')\n",
    "  csvwriter.writerow(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].scatter(epochs, loss, label = 'Training loss')\n",
    "axs[0].scatter(epochs, val_loss, label = 'Validation loss')\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "axs[1].set_title('Accuracy')\n",
    "axs[1].scatter(epochs, accuracy, label = 'Training accuracy')\n",
    "axs[1].scatter(epochs, val_accuracy, label = 'Validation accuracy')\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_NN = cnn_model.predict(x_test)\n",
    "prediction_NN = np.argmax(prediction_NN, axis=-1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, prediction_NN)\n",
    "print(cm)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  RANDOM FOREST PART\n",
    "#  Now, let us use features from convolutional network for RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_model = RandomForestClassifier(n_estimators = 50, random_state = 42)\n",
    "\n",
    "X_for_RF = feature_extractor.predict(x_train) #This will be input to RF\n",
    "\n",
    "# Train the model on training data\n",
    "RF_model.fit(X_for_RF, y_train) #For sklearn no one hot encoding\n",
    "\n",
    "#Send test data through same feature extractor process\n",
    "X_test_feature = feature_extractor.predict(x_test)\n",
    "#Now predict using the trained RF model. \n",
    "prediction_RF = RF_model.predict(X_test_feature)\n",
    "\n",
    "#Print overall accuracy\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix - verify accuracy of each class\n",
    "cm = confusion_matrix(y_test, prediction_RF)\n",
    "print(cm)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 25 mistakes\n",
    "error_idx = [i for i, rslt in enumerate(y_test) if (rslt + prediction_RF[i] == 1)]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(12,12))\n",
    "\n",
    "for i, j in [(i,j) for i in np.arange(5) for j in np.arange(5)]:\n",
    "    idx = error_idx[5*i+j]\n",
    "    image = cv2.cvtColor(x_test[idx], cv2.COLOR_HSV2RGB)\n",
    "    axs[i,j].imshow(image)\n",
    "    axs[i,j].set_title(f'pred {int(prediction_RF[idx])}, real {int(y_test[idx])}')\n",
    "    axs[i,j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
